{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "oqhRcXu3zUFU",
        "eAqyKujmzIHk",
        "YfG1eEO15Yob",
        "6gefHUyS_sSI"
      ],
      "authorship_tag": "ABX9TyNMMlFo2RCburGRTr/viMnu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmerkle/iannwtf_homework_group14/blob/main/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "oqhRcXu3zUFU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GSfRwi7_xfIM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "import urllib\n",
        "from IPython import display\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, Flatten, MaxPooling2D, Conv2DTranspose, Reshape, Activation, BatchNormalization, GlobalAvgPool2D"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data pipeline"
      ],
      "metadata": {
        "id": "eAqyKujmzIHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare(data, batch_size):\n",
        "  # reshape 1x784 pixel vectors to (28, 28, 1)\n",
        "  data = data.map(lambda img: tf.reshape(img, (28, 28, 1)))\n",
        "\n",
        "  #convert data from uint8 to float32\n",
        "  data = data.map(lambda img : tf.cast(img, tf.float32))\n",
        "  \n",
        "  #input normalization, bringing image values from range [0, 255] to [-1, 1] -> (img/128.)-1.\n",
        "  data = data.map(lambda img: (img/128.)-1.)\n",
        "  \n",
        "  #shuffle, batch, prefetch\n",
        "  data = data.shuffle(1000)\n",
        "  data = data.batch(batch_size)\n",
        "  data = data.prefetch(500)\n",
        "\n",
        "  #return preprocessed dataset\n",
        "  return data"
      ],
      "metadata": {
        "id": "aZPJl0V0zGLl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of a GAN"
      ],
      "metadata": {
        "id": "Nydc6iJ35TMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discriminator"
      ],
      "metadata": {
        "id": "YfG1eEO15Yob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    # , kernel_regularizer = tf.keras.regularizers.L2(0.001)\n",
        "    self.conv_1 = Conv2D(filters = 32, kernel_size = (3, 3), strides = 1, padding = 'same', activation='relu', input_shape=(28, 28, 1))\n",
        "    self.maxpool_1 = MaxPooling2D((2, 2), padding='same')\n",
        "    self.dropout_1 = Dropout(0.2)\n",
        "    self.conv_2 = Conv2D(filters = 64, kernel_size = (3, 3), strides = 1, padding = 'same', activation='relu')\n",
        "    self.maxpool_2 = MaxPooling2D((2, 2), padding='same')\n",
        "    self.dropout_2 = Dropout(0.2)\n",
        "    self.conv_3 = Conv2D(filters = 128, kernel_size = (3, 3), strides = 1, padding = 'same', activation='relu')\n",
        "    self.glob_pool = GlobalAvgPool2D()\n",
        "    self.dense = Dense(1, activation=\"softmax\")\n",
        "\n",
        "\n",
        "  @tf.function\n",
        "  def call(self, inputs, training):\n",
        "    x = self.conv_1(inputs)\n",
        "    x = self.maxpool_1(x)\n",
        "    if training == True:\n",
        "      x = self.dropout_1(x)\n",
        "    x = self.conv_2(x)\n",
        "    x = self.maxpool_2(x)\n",
        "    if training == True:\n",
        "      x = self.dropout_2(x)\n",
        "    x = self.conv_3(x)\n",
        "    x = self.glob_pool(x)\n",
        "    x = self.dense(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Xk1Mar8Z5YKr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator"
      ],
      "metadata": {
        "id": "6gefHUyS_sSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    # , kernel_regularizer = tf.keras.regularizers.L2(0.001)\n",
        "\n",
        "    self.dense_1 = Dense(7*7*256, activation=\"relu\", input_shape = (100,))\n",
        "    #self.batch_norm_1 = BatchNormalization()\n",
        "    self.reshape_1 = Reshape((7, 7, 256))\n",
        "\n",
        "    self.conv_t_1 = Conv2DTranspose(128, kernel_size = (4, 4), strides = 1, padding = 'same')\n",
        "    self.batch_norm_1 = BatchNormalization()\n",
        "    self.act_1 = Activation(tf.nn.relu)\n",
        "\n",
        "    self.conv_t_2 = Conv2DTranspose(64, kernel_size = (4, 4), strides = 2, padding = 'same')\n",
        "    self.batch_norm_2 = BatchNormalization()\n",
        "    self.act_2 = Activation(tf.nn.relu)\n",
        "\n",
        "    self.conv_t_3 = Conv2DTranspose(1, kernel_size = (4, 4), strides = 2, padding = 'same', activation='tanh')\n",
        " \n",
        "\n",
        "  @tf.function\n",
        "  def call(self, inputs):\n",
        "    x = self.dense_1(inputs)\n",
        "    x = self.reshape_1(x)\n",
        "    x = self.conv_t_1(x)\n",
        "    x = self.batch_norm_1(x)\n",
        "    x = self.act_1(x)\n",
        "    x = self.conv_t_2(x)\n",
        "    x = self.batch_norm_2(x)\n",
        "    x = self.act_2(x)\n",
        "    x = self.conv_t_3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "7quCoQzy5Smz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "LHwcqugJF1un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating images with generator\n",
        "def create_img(generator, batch_size):\n",
        "  noise = tf.random.normal([1, 100])\n",
        "  generated_image = generator(noise)\n",
        "\n",
        "  #plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
        "  return generated_image"
      ],
      "metadata": {
        "id": "y8SL-hJhG_5f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss of  discriminator is based on how well the discriminator detected fake images as fake and real images as real\n",
        "def get_discriminator_loss(bce_loss, real_prediction, fake_prediction):\n",
        "    # compute BCE between  generators output on fake images and all labels=0\n",
        "    fake_loss = bce_loss(fake_prediction, tf.zeros_like(fake_prediction))\n",
        "    # compute BCE between generators output on the real images and all labels = 1\n",
        "    real_loss = bce_loss(real_prediction, tf.ones_like(real_prediction))\n",
        "    #Add them both to receive the resulting loss of the discriminator\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "IcohO729Lbaf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss of generator is estimated by how well the generator was able to fool the discriminator\n",
        "def get_generator_loss(loss, fake_output):\n",
        "    return loss(tf.ones_like(fake_output), fake_output)"
      ],
      "metadata": {
        "id": "ijfJcivBLjtv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Train step*"
      ],
      "metadata": {
        "id": "_wBKXWlmik7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(generator, discriminator, true_images, bce_loss, discriminator_optimizer, generator_optimizer, batch_size):\n",
        "  # each training step generator is fed with random noise and creates images from it\n",
        "  generated_images = create_img(generator, batch_size)\n",
        "\n",
        "  # discriminator sees a batch of true images and a batch of the generated\n",
        "  with tf.GradientTape() as d_tape, tf.GradientTape() as g_tape:\n",
        "      real_prediction = discriminator(true_images, True)\n",
        "      fake_prediction = discriminator(generated_images, True)\n",
        "      # calculate losses\n",
        "      discriminator_loss = get_discriminator_loss(bce_loss, real_prediction, fake_prediction)\n",
        "      generator_loss = get_generator_loss(bce_loss, fake_prediction)\n",
        "      #Calculate accuracy\n",
        "      train_accuracy =  real_prediction == np.round(fake_prediction)\n",
        "      train_accuracy = np.mean(train_accuracy)\n",
        "\n",
        "  discriminator_gradients = d_tape.gradient(discriminator_loss, discriminator.trainable_variables)\n",
        "  generator_gradients = g_tape.gradient(generator_loss, generator.trainable_variables)\n",
        "\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
        "  \n",
        "  return discriminator_loss, generator_loss, train_accuracy"
      ],
      "metadata": {
        "id": "uVKJ1e_zF5Hg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Test step*"
      ],
      "metadata": {
        "id": "6PWSSfsbitC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(generator, discriminator, true_images, bce_loss, batch_size):\n",
        "  accuracy_aggregator = []\n",
        "  d_loss_aggregator = []\n",
        "  g_loss_aggregator = []\n",
        "\n",
        "  for image_batch in true_images:\n",
        "    generated_images = create_img(generator, batch_size)\n",
        "\n",
        "    real_prediction = discriminator(image_batch, False)\n",
        "    fake_prediction = discriminator(generated_images, False)\n",
        "    # calculate losses\n",
        "    discriminator_loss = get_discriminator_loss(bce_loss, real_prediction, fake_prediction)\n",
        "    generator_loss = get_generator_loss(bce_loss, fake_prediction)\n",
        "    #Calculate accuracy\n",
        "    accuracy =  real_prediction == np.round(fake_prediction)\n",
        "    accuracy = np.mean(accuracy)\n",
        "\n",
        "    d_loss_aggregator.append(discriminator_loss.numpy())\n",
        "    g_loss_aggregator.append(generator_loss.numpy())\n",
        "    accuracy_aggregator.append(np.mean(accuracy))\n",
        "\n",
        "  test_d_loss = tf.reduce_mean(d_loss_aggregator)\n",
        "  test_g_loss = tf.reduce_mean(g_loss_aggregator)\n",
        "  test_accuracy = tf.reduce_mean(accuracy_aggregator)\n",
        "\n",
        "  return test_d_loss, test_g_loss, test_accuracy"
      ],
      "metadata": {
        "id": "CJasEla0igjs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization"
      ],
      "metadata": {
        "id": "3noeoWPohnFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Description: This function visualizes the losses and accuracies of training and testing\n",
        "def visualization(train_d_losses, train_g_losses, test_d_losses, test_g_losses, train_accuracies, test_accuracies):\n",
        "  # Visualization of losses\n",
        "  plt.figure()\n",
        "  line1, = plt.plot(train_d_losses)\n",
        "  line2, = plt.plot(test_d_losses)\n",
        "  line3, = plt.plot(train_g_losses)\n",
        "  line4, = plt.plot(test_g_losses)\n",
        "  plt.title(\"Training and testing losses\")\n",
        "  plt.xlabel(\"Training steps\")\n",
        "  plt.ylabel(\"loss\")\n",
        "  plt.legend((line1, line2, line3, line4), (\"Training Discriminator\", \"Testing Discriminator\", \"Training Generator\", \"Testing Generator\"), fontsize=12)\n",
        "  plt.show()\n",
        "\n",
        "  #Visualization of accuracies\n",
        "  plt.figure()\n",
        "  line1, = plt.plot(test_accuracies)\n",
        "  line2, = plt.plot(train_accuracies)\n",
        "  plt.title(\"Training and testing accuracies\")\n",
        "  plt.xlabel(\"Training steps\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.legend((line1, line2),(\"Test accuracy\", \"Train accuracy\"), fontsize = 12)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "G8X4dbsIhpuJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution"
      ],
      "metadata": {
        "id": "nLF9Y5GEINhg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quickdraw get data"
      ],
      "metadata": {
        "id": "qTYTiQk2xq0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [line.rstrip(b'\\n') for line in urllib.request.urlopen('https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/categories.txt')]\n",
        "category = 'candle'\n",
        "\n",
        "# Creates a folder to download the original drawings into.\n",
        "# We chose to use the numpy format : 1x784 pixel vectors, with values going from 0 (white) to 255 (black). We reshape them later to 28x28 grids and normalize the pixel intensity to [-1, 1]\n",
        "\n",
        "if not os.path.isdir('npy_files'):\n",
        "    os.mkdir('npy_files')\n",
        "    \n",
        "url = f'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/{category}.npy'  \n",
        "urllib.request.urlretrieve(url, f'npy_files/{category}.npy')\n",
        "\n",
        "images = np.load(f'npy_files/{category}.npy')\n",
        "print(f'{len(images)} images to train on')\n",
        "\n",
        "# You can limit the amount of images you use for training by setting :\n",
        "train_images = images[:10000]\n",
        "# You should also define a samller subset of the images for testing..\n",
        "test_images = images[10000:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jUdwuSyxm5z",
        "outputId": "1d6b8d98-ff3e-4fd1-8e85-47a8eb95d09e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "141545 images to train on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data"
      ],
      "metadata": {
        "id": "n9rZxphcywHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "# Creates tensorflow datasets for the training and test data.\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(train_images)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(test_images)\n",
        "\n",
        "train_ds = prepare(train_ds, batch_size)\n",
        "test_ds = prepare(test_ds, batch_size)"
      ],
      "metadata": {
        "id": "W4FXZEC-y3M7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameters"
      ],
      "metadata": {
        "id": "GjahGfxOQKMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Hyperparameters\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "    \n",
        "# Initialize the optimizers: Adam\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Initialize the loss\n",
        "bce_loss = tf.keras.losses.BinaryCrossentropy()"
      ],
      "metadata": {
        "id": "_vYgtVKVQOlG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "YbTVdshdQKHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists for later visualization.\n",
        "train_g_losses = []\n",
        "test_g_losses = []\n",
        "\n",
        "train_d_losses = []\n",
        "test_d_losses = []\n",
        "\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "#testing once before we begin\n",
        "d_test_loss, g_test_loss, test_accuracy = test(generator, discriminator, test_ds, bce_loss, batch_size)\n",
        "test_d_losses.append(d_test_loss)\n",
        "test_g_losses.append(g_test_loss)\n",
        "test_accuracies.append(test_accuracy)\n",
        "\n",
        "#check how model performs on train data once before we begin\n",
        "d_train_loss, g_train_loss, train_accuracy = test(generator, discriminator, train_ds, bce_loss, batch_size)\n",
        "train_d_losses.append(d_train_loss)\n",
        "train_g_losses.append(g_train_loss)\n",
        "train_accuracies.append(train_accuracy)\n",
        "\n",
        "\n",
        "# We train for num_epochs epochs\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    #training (and checking in with training)\n",
        "    epoch_loss_agg_d = []\n",
        "    epoch_loss_agg_g = []\n",
        "    epoch_acc_agg = []\n",
        "    for image_batch in train_ds:\n",
        "        d_train_loss, g_train_loss, train_accuracy = train_step(generator, discriminator, image_batch, bce_loss, discriminator_optimizer, generator_optimizer, batch_size)\n",
        "        epoch_loss_agg_d.append(d_train_loss)\n",
        "        epoch_loss_agg_g.append(g_train_loss)\n",
        "\n",
        "    #track training loss\n",
        "    train_d_losses.append(tf.reduce_mean(epoch_loss_agg_d))\n",
        "    train_g_losses.append(tf.reduce_mean(epoch_loss_agg_g))\n",
        "    # tracking train accuracy\n",
        "    train_accuracies.append(tf.reduce_mean(train_accuracy))\n",
        "    \n",
        "    #testing, so we can track accuracy and test loss\n",
        "    d_test_loss, g_test_loss, test_accuracy = test(generator, discriminator, test_ds, bce_loss, batch_size)\n",
        "    test_d_losses.append(d_test_loss)\n",
        "    test_g_losses.append(g_test_loss)\n",
        "    test_accuracies.append(test_accuracy)    \n",
        "    \n",
        "visualization(train_d_losses, train_g_losses, test_d_losses, test_g_losses, train_accuracies, test_accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vyQOE0PSIRW3",
        "outputId": "645fa140-5631-4f75-9354-5fe388a39610"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-3ea80c420d98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mepoch_acc_agg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0md_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbce_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mepoch_loss_agg_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mepoch_loss_agg_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-457d216d3bbb>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(generator, discriminator, true_images, bce_loss, discriminator_optimizer, generator_optimizer, batch_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mdiscriminator_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mgenerator_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcross\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreplica\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     \"\"\"\n\u001b[0;32m--> 633\u001b[0;31m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_empty_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/utils.py\u001b[0m in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     raise ValueError(f\"No gradients provided for any variable: {variable}. \"\n\u001b[0m\u001b[1;32m     74\u001b[0m                      f\"Provided `grads_and_vars` is {grads_and_vars}.\")\n\u001b[1;32m     75\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: (['dense/kernel:0', 'dense/bias:0', 'conv2d_transpose/kernel:0', 'conv2d_transpose/bias:0', 'batch_normalization/gamma:0', 'batch_normalization/beta:0', 'conv2d_transpose_1/kernel:0', 'conv2d_transpose_1/bias:0', 'batch_normalization_1/gamma:0', 'batch_normalization_1/beta:0', 'conv2d_transpose_2/kernel:0', 'conv2d_transpose_2/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense/kernel:0' shape=(100, 12544) dtype=float32, numpy=\narray([[-0.02060771, -0.00392669,  0.02111815, ..., -0.01504002,\n         0.01652257, -0.01326767],\n       [ 0.01074952, -0.00896982, -0.00302594, ..., -0.02158397,\n        -0.0109423 , -0.02019322],\n       [ 0.01400694,  0.01628828, -0.01437287, ...,  0.01969311,\n        -0.00325537, -0.01255899],\n       ...,\n       [-0.01877041, -0.00500224, -0.00628175, ..., -0.00884716,\n         0.00355198,  0.00543092],\n       [ 0.0061385 ,  0.01390505,  0.00078497, ..., -0.00685744,\n        -0.01056795, -0.00458183],\n       [-0.01085825,  0.00325598,  0.00658245, ...,  0.00615007,\n        -0.00245708, -0.00541922]], dtype=float32)>), (None, <tf.Variable 'dense/bias:0' shape=(12544,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'conv2d_transpose/kernel:0' shape=(4, 4, 128, 256) dtype=float32, numpy=\narray([[[[ 2.34148055e-02,  1.27860084e-02,  3.04765254e-02, ...,\n          -3.01423818e-02, -1.44723803e-02, -1.07266530e-02],\n         [ 4.93471324e-03,  1.21185929e-03, -3.00501585e-02, ...,\n           3.05666849e-02,  3.05621549e-02, -2.01768652e-02],\n         [-1.29740089e-02, -9.12929326e-03, -3.03163975e-02, ...,\n           2.74476483e-02, -2.71469057e-02,  2.03832313e-02],\n         ...,\n         [-1.06147379e-02,  1.71468407e-02, -5.57221472e-04, ...,\n          -2.67235115e-02,  7.33708590e-03,  1.14554390e-02],\n         [ 2.25110650e-02,  2.33560577e-02,  1.41351223e-02, ...,\n          -2.92804986e-02, -6.21743500e-04,  2.26610079e-02],\n         [-1.20663345e-02, -1.41401440e-02, -2.71547362e-02, ...,\n          -9.96240228e-03,  8.07750970e-03,  6.05263561e-03]],\n\n        [[ 2.09483281e-02,  1.78392380e-02, -1.61225349e-02, ...,\n          -8.57492536e-03,  1.81041211e-02, -2.83134207e-02],\n         [ 2.07828656e-02,  2.70237252e-02,  8.21129978e-03, ...,\n           1.13969818e-02,  1.99176446e-02,  1.53312087e-03],\n         [-2.89848968e-02, -1.54232308e-02,  1.72216892e-02, ...,\n          -2.31225491e-02,  2.06041560e-02, -4.34739143e-03],\n         ...,\n         [-1.72412023e-02,  1.38602778e-02, -1.47389695e-02, ...,\n           2.71627828e-02,  2.64722705e-02, -2.50165537e-02],\n         [-1.34374052e-02,  1.30672827e-02, -2.43104845e-02, ...,\n          -5.48023731e-03,  1.62679553e-02,  9.18222219e-03],\n         [ 2.84329057e-04, -2.84512341e-03, -2.61145234e-02, ...,\n          -2.83093452e-02, -2.22438201e-02,  9.97785479e-03]],\n\n        [[ 6.39171153e-03, -3.84601206e-03,  9.48264450e-03, ...,\n          -3.30147147e-03, -2.57460102e-02,  7.38732517e-04],\n         [ 6.34669513e-03,  9.83521342e-03, -2.20446438e-02, ...,\n          -4.98881191e-03,  2.93901861e-02, -8.67878646e-03],\n         [ 1.30040646e-02, -2.79672444e-04, -1.24379098e-02, ...,\n          -7.83860683e-03, -1.27683952e-02,  1.47052333e-02],\n         ...,\n         [ 2.73389593e-02,  1.05704516e-02, -2.78866962e-02, ...,\n           1.96653083e-02, -2.07128972e-02,  2.88514569e-02],\n         [ 1.51849240e-02,  1.94896013e-02,  2.04866678e-02, ...,\n           1.05560273e-02,  7.56313652e-03, -7.25726038e-03],\n         [-9.90641862e-03, -7.81238824e-03, -1.18311718e-02, ...,\n          -1.63365081e-02, -2.86433771e-02,  2.98962742e-02]],\n\n        [[ 1.56205744e-02,  3.03022265e-02, -1.25897229e-02, ...,\n           7.83547014e-03, -2.82916799e-02, -1.31616667e-02],\n         [-1.37837455e-02,  2.21925303e-02, -1.39200911e-02, ...,\n           1.37022808e-02, -3.18735093e-03, -2.93937549e-02],\n         [ 1.50190592e-02, -2.10421532e-03,  9.28179920e-03, ...,\n           1.22022629e-03,  2.38397717e-03, -9.45959985e-03],\n         ...,\n         [ 2.87165418e-02,  2.81370357e-02, -1.36997923e-02, ...,\n           1.86593756e-02, -2.04820111e-02, -2.82785967e-02],\n         [ 1.58333257e-02, -1.44887716e-03,  1.27244666e-02, ...,\n           1.80211067e-02, -3.58826667e-03, -1.26582757e-02],\n         [-3.23903561e-03, -1.42941177e-02, -1.64718404e-02, ...,\n          -3.08987945e-02, -2.97334194e-02, -1.23164058e-02]]],\n\n\n       [[[-1.92571878e-02, -8.69303942e-03,  2.15797201e-02, ...,\n           3.04944664e-02, -1.25644803e-02,  2.19735578e-02],\n         [-2.98675150e-03,  2.21905932e-02,  1.19199604e-02, ...,\n           1.41385794e-02,  6.32379949e-03,  2.67553851e-02],\n         [-6.30970299e-03,  2.21299455e-02,  2.34553888e-02, ...,\n          -2.47221440e-03,  1.43240392e-03, -1.68319792e-03],\n         ...,\n         [ 1.69074386e-02, -2.85284072e-02, -5.05401939e-03, ...,\n           2.10948512e-02, -2.61462778e-02,  8.92612338e-03],\n         [-2.06822604e-02,  5.75383753e-03,  2.25753933e-02, ...,\n          -2.77145058e-02,  7.36173987e-03, -1.70843825e-02],\n         [ 7.10155070e-03,  1.54300705e-02,  3.08857039e-02, ...,\n          -4.14837152e-03,  9.25937295e-03,  2.30785161e-02]],\n\n        [[-1.45644248e-02, -1.11289620e-02, -2.86530927e-02, ...,\n           8.45871866e-04,  3.01325694e-02,  1.78882554e-02],\n         [ 2.27847397e-02, -1.84289142e-02,  9.25096869e-03, ...,\n          -2.00407803e-02, -2.85852700e-02, -1.47790462e-02],\n         [ 2.57248282e-02,  2.88435146e-02, -1.37027875e-02, ...,\n           5.76548278e-04, -1.02603137e-02,  1.46492198e-02],\n         ...,\n         [ 1.70230865e-02,  4.65531647e-03, -2.27915868e-02, ...,\n           8.08620453e-03,  9.71562415e-03,  2.22740471e-02],\n         [-3.26292962e-03,  1.61181167e-02,  1.82403624e-02, ...,\n           2.07453966e-03,  2.04786584e-02, -6.76821917e-03],\n         [ 2.47282460e-02,  7.47982413e-03,  2.58320570e-02, ...,\n          -8.00032914e-03, -2.35007405e-02, -2.81611830e-02]],\n\n        [[ 2.92095244e-02,  7.05601275e-03, -1.02586299e-02, ...,\n          -1.76856890e-02,  6.86809421e-03,  2.13362351e-02],\n         [-1.95226967e-02, -1.28957629e-02,  2.81374827e-02, ...,\n          -5.07923216e-03,  2.27220953e-02, -2.19992399e-02],\n         [ 1.68880001e-02, -1.44425035e-03,  2.21208632e-02, ...,\n           2.67355889e-03,  1.10258758e-02, -1.25889108e-02],\n         ...,\n         [-1.99978799e-03, -2.17751116e-02, -9.10237432e-05, ...,\n          -6.67922199e-03, -2.40827352e-02,  1.39951706e-03],\n         [ 5.54712862e-03,  1.74164921e-02, -2.45481580e-02, ...,\n           1.99688971e-02, -9.83782113e-04, -4.84624505e-03],\n         [ 2.06025392e-02,  3.00693586e-02, -1.64234638e-03, ...,\n          -5.40985167e-03, -6.46813959e-03,  5.45988232e-03]],\n\n        [[ 8.73749703e-03,  1.17135644e-02,  1.61649212e-02, ...,\n           2.73310989e-02, -5.82485646e-03,  1.23105943e-04],\n         [-2.94154212e-02, -5.33539802e-03,  1.34314448e-02, ...,\n           1.66806653e-02, -1.25580952e-02, -1.27496347e-02],\n         [-1.33076683e-02, -2.88145319e-02, -3.59202176e-03, ...,\n          -1.31952465e-02,  2.63886750e-02, -3.00198272e-02],\n         ...,\n         [-1.50163993e-02, -2.71631703e-02, -6.74518198e-03, ...,\n           1.17440075e-02,  7.02082366e-03,  1.73329711e-02],\n         [-6.34970516e-03,  1.86195150e-02,  4.20806557e-03, ...,\n           4.17412072e-03,  7.85145909e-03, -1.06646121e-03],\n         [ 1.67819634e-02, -2.43675709e-03,  2.87937373e-02, ...,\n           3.09353173e-02, -6.57119602e-03, -8.97645205e-03]]],\n\n\n       [[[-6.73021376e-03, -1.50328130e-03, -1.47785023e-02, ...,\n           2.62869373e-02, -3.02408040e-02,  6.49666786e-03],\n         [ 1.40544325e-02, -2.74405628e-02, -2.03457475e-03, ...,\n          -8.77950341e-03,  1.28900260e-03,  2.01556012e-02],\n         [ 1.45304129e-02,  1.09849498e-02,  1.94598362e-02, ...,\n           3.09955999e-02,  9.71071422e-04,  2.15515718e-02],\n         ...,\n         [-2.95116827e-02, -5.63138723e-03,  2.24749893e-02, ...,\n           2.56334618e-02,  2.10068002e-02,  2.43433043e-02],\n         [ 5.70037216e-03, -3.40027362e-03,  1.07818916e-02, ...,\n           1.96967572e-02,  1.75548121e-02, -3.45940888e-03],\n         [-1.38455033e-02, -7.76973367e-03, -1.15594491e-02, ...,\n           2.45781243e-03,  3.28926742e-03,  3.09824944e-04]],\n\n        [[-2.54303366e-02, -6.76047802e-03,  1.69377252e-02, ...,\n           2.02145576e-02, -7.21912086e-03,  2.42410451e-02],\n         [ 2.21047252e-02, -1.57879591e-02,  2.90152207e-02, ...,\n           5.66261262e-03,  1.26276538e-02, -2.81562135e-02],\n         [-1.67203695e-03,  2.81248242e-03,  2.63968557e-02, ...,\n          -1.92443281e-03,  2.17755884e-02, -2.08399817e-02],\n         ...,\n         [-1.99765712e-02, -3.10199633e-02,  2.16479748e-02, ...,\n           3.08206454e-02, -2.65408605e-02, -1.62450373e-02],\n         [-1.91351473e-02, -1.97941735e-02, -1.94001347e-02, ...,\n           1.20818242e-02,  1.12975687e-02, -9.59053636e-03],\n         [ 3.16362083e-03,  2.33103335e-02,  2.91670263e-02, ...,\n          -1.66952908e-02, -1.62552521e-02, -1.87843814e-02]],\n\n        [[-1.60959214e-02,  6.87440485e-03, -2.11733729e-02, ...,\n           9.61349905e-03, -2.98382342e-03,  2.08837390e-02],\n         [ 1.03873536e-02, -2.17583030e-03,  7.42300600e-03, ...,\n           1.19342357e-02, -1.75993368e-02,  2.48447582e-02],\n         [ 8.43793154e-03,  2.59203985e-02, -1.66308433e-02, ...,\n          -1.34253651e-02, -2.68174037e-02,  3.48687172e-04],\n         ...,\n         [ 2.48359889e-03, -1.78102031e-02,  2.55953148e-02, ...,\n           1.67970359e-03, -1.98755935e-02, -1.22740865e-04],\n         [ 1.90436840e-03, -5.94076514e-03,  2.61604786e-04, ...,\n          -1.07432753e-02,  1.98564902e-02, -2.27309093e-02],\n         [ 8.88495892e-03, -2.51113251e-02, -1.74208432e-02, ...,\n          -2.14380547e-02, -2.00639591e-02,  1.33165419e-02]],\n\n        [[-2.04243958e-02,  1.44416392e-02,  2.82756016e-02, ...,\n          -6.76991045e-03, -1.51601583e-02,  1.72838122e-02],\n         [-1.96343660e-03,  2.74474695e-02, -2.75128186e-02, ...,\n          -2.81465948e-02, -1.73658431e-02,  8.08048993e-03],\n         [ 4.25602496e-03,  5.24345785e-03,  2.79519632e-02, ...,\n          -9.71967727e-03, -2.37289220e-02, -1.94884241e-02],\n         ...,\n         [-1.12268478e-02,  5.85776567e-03,  6.74799085e-03, ...,\n           1.25367045e-02,  1.52187422e-02,  1.44230351e-02],\n         [-1.50558725e-02, -8.64811242e-04,  1.10116601e-03, ...,\n          -5.11658937e-03,  2.36909688e-02, -2.31500491e-02],\n         [ 6.78999722e-03,  1.65752321e-02,  2.62130052e-02, ...,\n           1.00157261e-02, -8.47356021e-03,  2.77386904e-02]]],\n\n\n       [[[ 1.05407611e-02,  1.31224468e-02, -2.52779573e-02, ...,\n          -2.07264572e-02,  2.37381384e-02,  2.13212743e-02],\n         [ 4.88524884e-03,  6.83595240e-03,  2.22832859e-02, ...,\n           3.07448059e-02,  1.25931948e-03, -1.48469135e-02],\n         [ 2.92431638e-02, -7.44739175e-03, -2.85347700e-02, ...,\n           9.46284831e-03, -1.68941915e-03, -2.66207233e-02],\n         ...,\n         [-4.64875251e-03,  2.05347985e-02,  1.26304552e-02, ...,\n           1.26846731e-02, -2.85740718e-02,  1.21994168e-02],\n         [ 1.25600547e-02, -7.37780333e-03, -2.78515369e-03, ...,\n          -1.04787052e-02,  2.81341746e-02, -2.68997177e-02],\n         [ 9.71284509e-03,  2.16264278e-03, -7.49289989e-03, ...,\n          -9.35839862e-03, -6.14332408e-03,  1.93177536e-02]],\n\n        [[-2.14352906e-02,  1.76578760e-05, -1.33977160e-02, ...,\n           1.53573155e-02, -1.51713192e-02, -6.72063231e-03],\n         [-2.57033482e-02, -2.19219476e-02,  2.12649927e-02, ...,\n           4.32170182e-03,  2.96686664e-02,  2.05459669e-02],\n         [-1.84497535e-02, -2.52715126e-02, -5.06798923e-03, ...,\n           1.05454847e-02,  1.34158358e-02,  3.05563956e-02],\n         ...,\n         [-3.04403976e-02,  2.59871706e-02, -2.93893293e-02, ...,\n          -2.49698013e-02,  8.80514085e-03, -1.30181015e-02],\n         [ 2.58284062e-02, -3.06819677e-02,  2.17736885e-02, ...,\n           2.18687430e-02,  1.07252970e-02, -8.65201652e-03],\n         [-1.49033293e-02,  1.92924216e-02,  1.82657689e-02, ...,\n           8.12569261e-03, -2.75488794e-02,  1.57363415e-02]],\n\n        [[ 3.77646834e-03, -8.49615782e-03,  2.43013725e-02, ...,\n          -2.84010470e-02,  2.05271691e-03, -2.41047516e-02],\n         [ 1.98561102e-02, -1.34183690e-02,  1.05232745e-02, ...,\n          -3.60861421e-03, -2.91464776e-02, -2.03049853e-02],\n         [-1.59187615e-03, -5.65946102e-03,  4.84113395e-03, ...,\n           7.04299659e-03, -1.55729577e-02,  1.90499797e-02],\n         ...,\n         [-9.50215757e-03,  1.33652613e-02, -2.10338011e-02, ...,\n           1.39574707e-02,  1.40981972e-02,  9.16412473e-03],\n         [-1.13744065e-02, -1.94857791e-02, -5.13955206e-03, ...,\n          -2.05075368e-02, -2.60602161e-02, -1.58146024e-02],\n         [ 2.38191560e-02, -2.56218538e-02,  1.00197122e-02, ...,\n           8.18767399e-03, -3.17725539e-03, -2.09901258e-02]],\n\n        [[ 1.15303174e-02,  3.69504839e-03, -2.89353877e-02, ...,\n           9.20907408e-03,  4.73762304e-03,  3.12139094e-02],\n         [ 5.78328967e-03, -4.87443060e-03,  4.42698598e-03, ...,\n          -2.04683989e-02,  8.02166015e-03, -2.27318108e-02],\n         [ 1.85916573e-03, -2.35682204e-02, -8.40180367e-03, ...,\n          -2.65840739e-02, -1.46210641e-02, -1.76607445e-02],\n         ...,\n         [-5.60931861e-03, -4.49237227e-03,  2.07884386e-02, ...,\n          -3.98810208e-03,  3.07288617e-02,  7.91687518e-03],\n         [-1.55765489e-02, -1.66881457e-02,  5.47154993e-03, ...,\n          -3.64734977e-03,  2.07139924e-02,  2.65171304e-02],\n         [-7.40564615e-03, -1.57634169e-02, -2.54766941e-02, ...,\n           6.40057027e-03,  1.56205148e-03, -1.56821385e-02]]]],\n      dtype=float32)>), (None, <tf.Variable 'conv2d_transpose/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'batch_normalization/gamma:0' shape=(128,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>), (None, <tf.Variable 'batch_normalization/beta:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'conv2d_transpose_1/kernel:0' shape=(4, 4, 64, 128) dtype=float32, numpy=\narray([[[[ 0.01036163,  0.00574256,  0.02713307, ..., -0.00568853,\n           0.03476544,  0.00089826],\n         [ 0.01706901,  0.02616191, -0.01539384, ..., -0.01575486,\n           0.03679096, -0.04387738],\n         [ 0.00812954,  0.03979846, -0.01873041, ..., -0.01100355,\n          -0.02603777, -0.01917225],\n         ...,\n         [ 0.02433384, -0.01062278, -0.0255418 , ..., -0.03210436,\n           0.04305919, -0.03795684],\n         [ 0.00968263, -0.01750228,  0.02978036, ...,  0.01927743,\n          -0.00984215, -0.04085721],\n         [ 0.03097719,  0.01374159,  0.03621935, ...,  0.00968347,\n           0.03047166,  0.01942005]],\n\n        [[-0.00235255, -0.01768935, -0.02659485, ...,  0.03052857,\n           0.03742963,  0.00374346],\n         [ 0.04290752, -0.04358574, -0.01790568, ...,  0.01415398,\n          -0.02839907,  0.00073204],\n         [-0.03614941, -0.01594404, -0.01342182, ...,  0.0410979 ,\n          -0.02000105,  0.00769515],\n         ...,\n         [ 0.0387729 ,  0.00987263, -0.04092475, ...,  0.01291909,\n          -0.0187504 , -0.02669627],\n         [-0.01400512, -0.00731637, -0.03482827, ..., -0.02612617,\n          -0.0170187 , -0.01800204],\n         [ 0.01388483,  0.02078232,  0.03949888, ...,  0.03720715,\n          -0.0131823 ,  0.01594616]],\n\n        [[ 0.03979894, -0.03674184,  0.03786775, ...,  0.04218802,\n           0.04136268, -0.03385925],\n         [ 0.01609081,  0.00654517,  0.02216179, ...,  0.02034368,\n           0.01203395,  0.04165807],\n         [ 0.02141209,  0.02754511, -0.02518798, ..., -0.00352667,\n          -0.01993661, -0.01405612],\n         ...,\n         [ 0.00803396,  0.04382845,  0.03248973, ..., -0.03810287,\n           0.01056658,  0.03455487],\n         [-0.02358855, -0.02737516,  0.02518138, ...,  0.00383247,\n          -0.01788414, -0.00641434],\n         [-0.01796997, -0.00226789, -0.01010703, ...,  0.00927589,\n          -0.04167105, -0.01504754]],\n\n        [[ 0.03712044, -0.04236514,  0.01811463, ...,  0.00193024,\n          -0.03456043,  0.00162445],\n         [ 0.01339857, -0.0247556 ,  0.02694384, ..., -0.03560805,\n           0.00584035, -0.02830555],\n         [ 0.01665904, -0.03514232, -0.02605274, ...,  0.04200288,\n           0.01298866, -0.03113407],\n         ...,\n         [ 0.04333117,  0.03387426, -0.01947827, ...,  0.01823828,\n          -0.01023437,  0.01325448],\n         [ 0.01983142, -0.01857931, -0.04324393, ...,  0.01160797,\n           0.03791464, -0.01045043],\n         [-0.0175328 ,  0.02813319, -0.02644028, ...,  0.0333365 ,\n          -0.02322763, -0.01662398]]],\n\n\n       [[[-0.01990323, -0.02865619, -0.0360182 , ...,  0.03931445,\n           0.03814191, -0.00217123],\n         [-0.00953129,  0.01167719, -0.02771232, ..., -0.04072371,\n           0.00565867,  0.01579101],\n         [-0.0018717 ,  0.03190994,  0.02980738, ...,  0.0143343 ,\n           0.01329943,  0.01788351],\n         ...,\n         [ 0.00075785,  0.02797573,  0.0413616 , ..., -0.00319839,\n          -0.0364471 ,  0.01539461],\n         [-0.00107379, -0.00142812, -0.01570734, ..., -0.0335892 ,\n          -0.02971378,  0.01442355],\n         [-0.03980372, -0.04222013, -0.02105666, ..., -0.00225764,\n           0.00926568,  0.04398704]],\n\n        [[ 0.00062595, -0.01733119, -0.03379983, ..., -0.02743207,\n           0.03804108,  0.01637114],\n         [ 0.03957649,  0.03471676, -0.00979067, ...,  0.01538171,\n          -0.00399135, -0.00664593],\n         [-0.04317346, -0.01279346,  0.04061219, ..., -0.00519366,\n          -0.03211795, -0.01725474],\n         ...,\n         [ 0.02598894,  0.04016269, -0.02486158, ..., -0.01524622,\n           0.01901111, -0.03575712],\n         [ 0.04141995, -0.0049415 , -0.03520739, ...,  0.01695611,\n          -0.0120241 , -0.0403302 ],\n         [-0.0422062 , -0.0055404 ,  0.03620609, ..., -0.01525871,\n          -0.01626673,  0.00369122]],\n\n        [[-0.02297604,  0.0433774 , -0.03086108, ..., -0.03366794,\n          -0.01280194,  0.03230494],\n         [-0.04366477,  0.00870254,  0.03802134, ...,  0.04393583,\n          -0.02277679,  0.03571214],\n         [ 0.02978415,  0.00445194,  0.00255774, ..., -0.03135518,\n          -0.00063059,  0.04032884],\n         ...,\n         [-0.0395053 , -0.00439973,  0.01226058, ..., -0.02407568,\n           0.02299166,  0.03521847],\n         [-0.02036126, -0.03120355,  0.01382088, ...,  0.0010115 ,\n           0.01907897,  0.01718066],\n         [-0.02427076, -0.00855952,  0.02906358, ..., -0.01006311,\n           0.03501499, -0.00660568]],\n\n        [[ 0.01399269,  0.00841287, -0.03819492, ...,  0.01830104,\n           0.0407344 ,  0.01501704],\n         [ 0.02440045,  0.01552375,  0.02984734, ...,  0.00914594,\n          -0.01095858, -0.02444627],\n         [ 0.03308189,  0.01905921,  0.02702862, ...,  0.02184742,\n          -0.03711433,  0.00108816],\n         ...,\n         [ 0.00147555,  0.02787748, -0.02231653, ...,  0.02933054,\n          -0.00627584, -0.02253672],\n         [-0.04405466,  0.03434702, -0.02414032, ...,  0.02022171,\n           0.00736972, -0.01663472],\n         [-0.02549107,  0.00167551, -0.00786984, ..., -0.03607156,\n           0.00702749, -0.00250649]]],\n\n\n       [[[ 0.04085943,  0.00473719,  0.01381844, ..., -0.02763429,\n          -0.03143108,  0.04306044],\n         [ 0.02891863,  0.03353555,  0.00080662, ...,  0.0046657 ,\n           0.01529467, -0.0047113 ],\n         [-0.01768322,  0.00527466, -0.03822978, ...,  0.03608671,\n          -0.04186713, -0.03992204],\n         ...,\n         [ 0.0134385 , -0.01179812,  0.00690692, ...,  0.0027632 ,\n           0.0298536 , -0.03771982],\n         [ 0.03721346, -0.02063908,  0.03534405, ...,  0.02459345,\n           0.0287471 , -0.03617603],\n         [ 0.00339993, -0.0321996 , -0.04098659, ..., -0.0256805 ,\n          -0.04183083, -0.02094843]],\n\n        [[ 0.03422989, -0.02333082, -0.0136639 , ...,  0.03150182,\n          -0.02979356, -0.04091795],\n         [ 0.01078755, -0.02634153, -0.00192587, ..., -0.03140963,\n           0.03572259, -0.02290269],\n         [-0.0012847 ,  0.01816006,  0.01236856, ...,  0.00313442,\n           0.0217861 ,  0.0059582 ],\n         ...,\n         [ 0.0218209 ,  0.04383207, -0.01476745, ..., -0.03158571,\n          -0.02689594, -0.01636656],\n         [-0.04266814, -0.03141201,  0.01556124, ...,  0.03024598,\n           0.017764  ,  0.01052584],\n         [-0.00381841,  0.04227763,  0.01021216, ..., -0.02003342,\n           0.00116068, -0.03578033]],\n\n        [[ 0.03353474,  0.00116809, -0.01357965, ..., -0.03698409,\n          -0.00927783, -0.01474878],\n         [-0.02846176, -0.00453263,  0.02126345, ...,  0.02170469,\n           0.02552145,  0.02084691],\n         [-0.00958709, -0.02951936, -0.03317842, ...,  0.02749293,\n           0.02473075,  0.02680178],\n         ...,\n         [ 0.01040328, -0.03205867, -0.01931136, ...,  0.02331964,\n           0.04025304, -0.01511579],\n         [-0.04252599,  0.01310823, -0.01396125, ...,  0.03849072,\n          -0.02262929,  0.01886627],\n         [ 0.00358594, -0.02580889,  0.02698237, ..., -0.0397421 ,\n           0.02532738,  0.03327828]],\n\n        [[-0.01211849, -0.04213506,  0.01167154, ...,  0.00015522,\n          -0.02023877,  0.02773231],\n         [ 0.02087934,  0.01567126,  0.04380218, ...,  0.01050812,\n           0.03987493, -0.01279272],\n         [ 0.02438494,  0.00509888,  0.01452985, ...,  0.04411004,\n          -0.02985705, -0.04047584],\n         ...,\n         [ 0.02000032, -0.04122248,  0.03923913, ..., -0.02596075,\n          -0.02067774,  0.01918478],\n         [-0.04149093,  0.00369012,  0.01812274, ...,  0.03502489,\n           0.01259836,  0.03453127],\n         [ 0.01313863,  0.03903333,  0.03709337, ..., -0.02301653,\n          -0.01055091,  0.00984119]]],\n\n\n       [[[ 0.04171846, -0.0105003 , -0.01865736, ...,  0.03361463,\n          -0.02502752,  0.00036656],\n         [-0.03599702,  0.02755655,  0.0419006 , ..., -0.01536612,\n           0.01404864,  0.0261492 ],\n         [ 0.02069544,  0.01986356, -0.00161518, ...,  0.0012568 ,\n          -0.00309715, -0.03701961],\n         ...,\n         [ 0.00385103, -0.02222736, -0.00312549, ...,  0.0083976 ,\n          -0.0411245 , -0.03525371],\n         [-0.03795349,  0.00259292,  0.0363491 , ..., -0.02555257,\n          -0.01833995,  0.00492223],\n         [-0.0276726 ,  0.00864862, -0.00092619, ...,  0.00502878,\n          -0.02255695,  0.00780908]],\n\n        [[ 0.02574022,  0.02203494,  0.00642907, ..., -0.04074313,\n           0.00368387,  0.04169831],\n         [-0.04306655,  0.03340686,  0.00450568, ...,  0.01482055,\n          -0.03547779,  0.02820333],\n         [ 0.02237334, -0.00411728, -0.00165796, ...,  0.00320493,\n           0.02531764,  0.02632624],\n         ...,\n         [ 0.02286065, -0.02949557,  0.01913188, ..., -0.0230927 ,\n          -0.03008735,  0.0039737 ],\n         [ 0.0202295 ,  0.04046447,  0.00032637, ..., -0.04289496,\n          -0.00871591, -0.03540915],\n         [ 0.02659127, -0.03529894,  0.04372418, ...,  0.02747501,\n          -0.02047202, -0.04386444]],\n\n        [[-0.0046017 , -0.02691596,  0.02244748, ..., -0.03360302,\n          -0.02652924,  0.01110847],\n         [-0.03234772, -0.00393165, -0.00894881, ..., -0.00036072,\n          -0.02960957,  0.01822674],\n         [ 0.04413133, -0.00486176,  0.01229103, ...,  0.02522941,\n           0.0179358 , -0.04105474],\n         ...,\n         [-0.02569854,  0.03598067,  0.02551911, ...,  0.02985584,\n          -0.04339998, -0.00536432],\n         [ 0.01776046, -0.03615475,  0.00069851, ...,  0.02349978,\n           0.02102966,  0.01688552],\n         [ 0.01774812, -0.00287857, -0.02175105, ..., -0.02275855,\n           0.00484277,  0.0434059 ]],\n\n        [[ 0.02514766, -0.0215272 , -0.04183913, ...,  0.01535562,\n          -0.0222468 ,  0.02664192],\n         [ 0.01578014, -0.02455534, -0.04239252, ..., -0.03780225,\n          -0.00889306,  0.02182217],\n         [-0.01023753,  0.0242781 ,  0.03654898, ...,  0.02769705,\n           0.0042465 ,  0.02225383],\n         ...,\n         [ 0.0388451 , -0.0194503 ,  0.04261037, ..., -0.01783332,\n           0.04269623, -0.0129988 ],\n         [-0.03414367,  0.0054389 ,  0.01170259, ...,  0.01122001,\n           0.00208556,  0.03077075],\n         [ 0.00025602,  0.00973557, -0.00708506, ..., -0.0075072 ,\n           0.00189203,  0.00336659]]]], dtype=float32)>), (None, <tf.Variable 'conv2d_transpose_1/bias:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'batch_normalization_1/gamma:0' shape=(64,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>), (None, <tf.Variable 'batch_normalization_1/beta:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'conv2d_transpose_2/kernel:0' shape=(4, 4, 1, 64) dtype=float32, numpy=\narray([[[[ 0.06161273, -0.02521303, -0.02789187, ...,  0.04903726,\n          -0.01577285,  0.02130043]],\n\n        [[-0.05441227, -0.05582263,  0.05868326, ..., -0.02110414,\n          -0.07176612, -0.07563788]],\n\n        [[-0.03251978, -0.04950026,  0.03817209, ..., -0.07158771,\n          -0.03537741, -0.01320262]],\n\n        [[-0.06909937, -0.0074001 , -0.04314685, ...,  0.02820244,\n          -0.02158378, -0.03777152]]],\n\n\n       [[[ 0.05892743, -0.0406325 , -0.04528471, ...,  0.0077309 ,\n           0.0623737 ,  0.06727584]],\n\n        [[ 0.04357736,  0.01912193, -0.05747069, ...,  0.059064  ,\n          -0.0577466 , -0.06598886]],\n\n        [[ 0.04903134, -0.04912949,  0.05366126, ..., -0.03012726,\n          -0.07423388,  0.01485555]],\n\n        [[-0.01411756,  0.01974954,  0.04431004, ...,  0.070005  ,\n          -0.06732434,  0.03627776]]],\n\n\n       [[[-0.01584283, -0.0320522 , -0.04346416, ..., -0.03657115,\n           0.06640023, -0.05251285]],\n\n        [[ 0.02953743, -0.01997628,  0.04914151, ..., -0.02612711,\n           0.0573508 ,  0.06960791]],\n\n        [[-0.05868618,  0.06970125,  0.02527205, ...,  0.05416998,\n           0.06918207, -0.06865977]],\n\n        [[-0.00037642,  0.00777549, -0.01926847, ..., -0.04055139,\n          -0.01501737,  0.01707862]]],\n\n\n       [[[-0.01631948,  0.04629283, -0.044445  , ...,  0.03388914,\n           0.0238044 , -0.0227007 ]],\n\n        [[-0.05191168, -0.00591742,  0.07453962, ...,  0.00480118,\n           0.02202851, -0.03404425]],\n\n        [[-0.06603147, -0.00940557, -0.00970329, ...,  0.04109614,\n          -0.04921138, -0.03507319]],\n\n        [[-0.04778577,  0.05592753, -0.00022323, ...,  0.03422076,\n           0.01606202, -0.03795089]]]], dtype=float32)>), (None, <tf.Variable 'conv2d_transpose_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>))."
          ]
        }
      ]
    }
  ]
}